Tim Visher contacted me and asked me to share my experience building and
deploying services with Clojure.

Tim and I worked together at Relay Network, and Tim has been contrasting his
experiences before we worked together (in an Enterprise Shop), and after we
worked together (when Tim gained experience working at tech start-ups).

Working Titles

   "Are you converging on an O(1) software process?"
   "How we move Clojure from Development to Production"

Some of the themes we want to roll up into a talk are:

* Holistic Consistency
   Make Dev and prod as similar as possible.
* Convention over Configuration
   Support configuration so you get loose coupling and late binding, but don't require it.
* Unorthodox, but consistent, choices reduce effort of development deployment and introspection
   Anatomy of a @-main@
* Service Registries are Complex and Cost Time
   you will want etcd or zk at some point, make that possible but don't force it a-priori
* Clojure's Live Development is Valuable, our decisions should facilitate using it
* Communication in your team: ephemeral diagrams as a tool for model alignment

What are the key take-aways I want people to get out of the talk?

* There are some unique things about Clojure (Lisps) that facilitate productivity
  * The mythical 'REPL', but that's not exactly what I do (Philip Fominykh)
* Consistency across your environments simplifies development and troubleshooting
  * Run the REPL in the service
  * Abstract away configuration early, but make it simple (json file from a known place)
  * Conventions for Port Assignments
    * lets you run the full stack on 1 box
    * eliminates or reduces bespoke configuration when spinning up new environments
      you don't have to stop and think "what port will I use" - which leads to both
      configuring the current service, but the consumers too, which leads to thinking
      "oh, I need a service registry service", (blech!)
* My patterns for developing Clojure applications
  * Productivity == Iteration
    The faster you can make it to iterate the faster you'll build your applications.  
    Anything that 
  * make a main that runs the repl at a minimum
  * step out of the path of complex configuration by just using clojure to start your 
    web server, thread pools, resource pools and complex object graphs directly.
    Why invent or use a complex XML or Json configuration document along with validation,
    and semantics for how it's processed?  You'll just end up burning time extending it
    as you run into new use-cases.  Why go through the effort of selecting, let alone
    integrating a DI framework when you can just (load-file)
  * command line / thick client apps
  * web services

What is productivity?
  Iteration, iteration, iteration
  not breaking your flow
    Be allergic to the things that break your flow, develop a sensitivity to
      having your time wasted.
  O(1) access to what you need in the moment

Anecdote: "Triggering my timeout" 

  I was pairing with a developer who had recently joined my team.  To keep them
  anonymous, we'll call her Mrs T.  Our task was to introduce a front-end web
  build into our application.  Assemblage, concatenation and minification of our
  front-end assets - html, javascript, and css.

  Mrs T. had previously used an Ant based build tool, I think the name of the
  project was html5-boilerplate.  So we did a short spike on getting it set up
  for our project.  The first time we ran the tool it took like 20 or 30 seconds
  for Ant (the JVM) to start up and begin assembling the assets.  

  I became very visibly agitated at this.  My pair was a bit taken aback at my
  reaction.  Mrs T. had come from an environment where they had taken an 8 hour
  build down to 45 minutes for a large complex application, so a tool that took
  90 seconds to run was very reasonable.  

  My reaction was the opposite, to me anything that took more than a small
  handful of seconds, especially something we would have to run repeatedly as
  part of our development process (iterating on a task) was going to break our
  flow and seriously impact our ability to iterate.  To me 90 seconds for
  something we were going to have to do over and over was a deal breaker.

  90s between me making a change and being able to run the code?  No way jose!

HERE 

Q: Should I talk about these topics?
  * pairing in the talk?
  * our workstation setup in the talk?
  * our estimation and task breakdown process?
  * talk about the importance of the shared model?  Continuously drawing diagrams to foster understanding?
  * ubiquitous 3x5 cards and pens anywhere a developer (or pair) might be working?


Considerations:

Where possible discuss the key trade offs or considerations that lead to my choices.


Dynamic infrastucture, adaptive scaling, automatic service discovery and registration.

h3. Holistic Considerations

The same pattern applied to each service.  Nothing new to keep in mind when working on different services, they all have an NRepl, a public API and a private API.  To develop on each of them, you take the same steps, you clone and run the @-main@.

h3. Convention over Configuration

    We chose a model for the ports our services listen on, though we support configuration.
    This allows us to run our full stack of services on a single system without having
    to do any explicit configuration.  This same convention is followed in our CI, QA, UAT
    and production environments even though they are multi-system.

Configuration comes from "somewhere", for us that is JSON files.  That's behind an interface that treats it as a K/V store, so we can pretty easily replace that with someting shared (like a zk or etcd service).  Haven't done that yet, and that abstraction would still let us do dev against the JSON config while prod would be vs the config service.

Externalize configuration from the start.  Don't force a registry, use simple text files but eliminate complexity by choosing conventions.  

Discuss specific examples here: DI concerns like database connections, serivce location (amqp, web services, etc).

Port Assignments

each service has 

* an NRepl for control
* a public web port (optionaly a public facing HTTP api)
* a private web port (optionaly a internally facing HTTP api)

NRepl (rather swank) ports are allocated in the 4000-4999 range by the team
agreeing on a vlaue for the new (logical) service that doesn't conflict with
our other services.  

The public web port is in the 8000-8999 range.

The prvate web port is the public port +1.

For example, service A uses 4010, 8010 adn 8011.  Service B uses 4020, 8020 and 8021.  The serivces support these values being overriden via configuraiton but default to these values.  This means developers and ops engineers don't need to even consider configuration when attempting to run mulitple services on one system.

Default hostnames are 'localhost', this is very convienient for Dev, QA, CI.  It's still pulled from configuration though, so prod just fills the slots in with hostnames that point to load balancers or hostnames that map to Virtual IPs (EIPs) configured for HA/Fail-over.

Consideration: dynamic reconfiguration

h3. Anatomy of a @-main@

* bootstrap configuration
* launch NRepl
* configure DI resources (db pools, http apis, other initialization)
* start in-JVM services (Quartz, MQ thread pools)
* start HTTP / RPC services
* announce the service is up

h3.  Unorthodox, but consistent, choices reduce effort of development deployment and introspection

We support runing an NRepl in Dev, CI, QA and prod.  Our service's main starts the NREPL.  We run our services by executing the @-main@ in all these environments.  If your service starts successfully in dev it is very likeley to also start successfully in all other environments.

This helps minimize the differences between debugging in dev and production.

h3. Communication in your team: ephemeral diagrams as a tool for model alignment

This may seem out of place in this talk but I feel it's a key aspect of
maintaining consistency.  Consistency is enforced through culture.  

Most things in computing are abstract, they can't be seen.  I can't point at
our infrastucture in AWS or at how our services interact with each other.  

Humans are very spacially oriented in our thinking.  You're receiving
information during this talk, which is very abstract, but you're listening to
me and you're lookng at me...at least I hope you are..you might not be (point
at someone) really looking at me, but spacially you're oriented in one
direction.

Spacial reference helps us communicate.

We draw diagrams of our software, lots of them.  Typically on three by five
cards, many times a day.  We sometimes draw several in a row, ripping up and
crumpling the 3x5 cards up as we refine the diagrams.  

We don't preserve these on purpose.  They are ephemeral.  They take seconds to
draw or re-create, so there is little reluctance to making them.  If they're
not quite right (and they often are not), ok, rip it up and make another.

No one diagram can be right.  We draw these at a level of abstraction showing
only the aspects necessary for the conversation at hand.  

The fact that we practice these so often means our team (all 7 of us) each has
a good understanding of our service interactions and how our environments are
layed out.  Troubleshooting typically begins with two engineers drawing a diagram
so they can start to bisect the problem space.

This is a pretty easy thing to bootstrap in your team, and if you have 3x5
cards availble in O(1) access you can do it with very little effort.

I've found this to be a great tool for bootstraping indepence for our engineers.





Stream of consciousness:

tv: 'organize and build your services so that devs move fast and deployment is invisible

tv: Is another theme here that late dep resolution isn't a thing? RE
    no maven resolution in prod. (accomplished _not_ by uberjar but by
    manual jar management)

    This is a concern both for runtime deps and for build artifacts.
        - Why have your ci process do a build when you can just check things
          in?
        - Why have your environmental config be embedded in the code at all?

          I say this one because that's I think a common response. You
          don't check build artifacts in because build artifacts are
          environtment specific. They shouldn't be.

    10:00 i have over and over again told people about checking in your build/dependency artifacts and the universal reaction is entirely negative
    10:00 but you guys don't do that
    10:00 is that somehow part of the system there?

tv: Is another theme the timeout? Deploys at RJ take several minutes.
    Not much more than at Relay, iirc. At Monetate they took hours and
    hours. At Siemens they took weeks.

tv: Thesis statement candidate: Consistency is the key to stability.

    9:47 so, thesis: consistency is the key to stability
    9:47 ?
    9:47 is that the common thread in all of relay's structure?
    9:48 consistence start up interface
    9:48 consistent ports between environments
    9:48 consistent tooling (shell based, pipeable, etc.)
    9:48 ?
    9:48 consistency is the key to ease?



TODO: slide / graffle diagrams: show the interacting services on 1 box and distributed across a cluster, by using conventions for port assignments and configuration for hostnames (that defualt to localhost, which we can use a local nginx configuraiton to abstract if we really want to), the services can run on 1 box (full stack for devs) or across a cluster (prod, qa, uat)

