Polyglot Persistence w/Neo4j

  Mark
  Praveena


Options for importing data into neo4j

  http://bit.ly/_import
   => https://docs.google.com/document/d/1dECIA-Q0qR8SLJu5WmFuR6Vl40PgnDtqb1CBvffgfL4/edit

   . first dataset from this doc is a stackoverflow sample

LOAD CSV via Cypher

* for well formatted csv data
* not as fast as other methods
* works w/transactions

Cypher and APOC

* Awesome Procedures On Cypher
* plugin mechanisim / procedures
* 'CALL PROC-NAME'
* 'FN-NAME()'
* APOC library brings togethr a set of common tools for import, eg: iterating &
  batching of transactions
* Hepers for: GraphML, JDBC, XML, JSON, & others.

Procedures

* write you own!  In Java, or anything that can compile into a jar
* place in plugins folder
* these will be via the transaction log
* fine grained access control, who can use, etc.

Driver via BOLT

* bolt is the protocol for connecting applications to neo4j service, custom
  wire protocol for graph data
* cypher queries -> stream of results
* .NET, Java, JavaScript and Python are officially supported
* Transactional, Batching & Parallelization possible
* Unofficial for PHP and Golang

./bin/neo4j-admin import

* fastest method -- skips the transaction log!
* initial import, a new database is created!
* db is offline during import, data must be specially prepared!
* cluster must be synchronized AFTER import!

Q[guy w/Airborne hat]: when will you support multiple databases per instance
>> NB: no schemas or multi-tenancy?
>> working group witin cypher, 'multigraph problem'
>> "at the db level, it's all designed for multiple databases'

NB: import is safe, if the destination exists, it'll refuse unless you specify `--force`

BatchInserter

* initial import or update (yay)
* application uses the batchloader API
* one of the faster import methods & more effort than the other approaches

neo4j ETL
* offline tool
* see: https://neo4j.com/blog/rdbms-neo4j-etl-tool/


Cypher and load CSV
  * https://neo4j.com/docs/cypher-refcard/current/

    LOAD CSV
    WITH HEADERS    // use first row as the header, rows are treated as a map
    FROM "url"      // file:// url relative to $NEO4J_HOME/import or http:// (??? or https://)
      AS row        // each line of CSV will be treated as a list of strings or map
                    //   NB: 'row' is a name we choose for the statement
    ...             // remainder of Cypher statement


    [USING PERIODIC COMMIT]  // optionally batch transactions

    ...
    AS row
    [FIELDTERMINATOR ";"] // optionally specify the delimiter
    ...             // remainder of Cypher statement



    LOAD CSV WITH HEADERS FROM "(file|http)://" AS row
    CREATE (:Label {property: row.header})
     MATCH (:Label {property: row.header})  // <- OR
     MERGE (:Label {property: row.header})  // <- OR

    // where :Label is the entity type eg: :ID(Person)


Jump to http://localhost:7474/browser/, then enter into the cypher box:

  :play https://guides.neo4j.com/import
  :clear

interactive tutorial / guide

  LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/neo4j-contrib/training/master/modeling/data/flights_initial.csv" AS row
  RETURN row
  LIMIT 5



  LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/neo4j-contrib/training/master/modeling/data/flights_initial.csv" AS row
  // merge the Airport (origin and dest) since that code is highly denormalized
  MERGE (origin:Airport {code: row.Origin})
  MERGE (destination:Airport {code: row.Dest})
  // NB: there is no unique identifier for a flight, so we'll attempt to make one here
  WITH row.UniqueCarrier + row.FlightNum + "_" + row.Year + "-" + row.Month + "-" + row.DayofMonth + "_" + row.Origin + "_" + row.Dest AS flightIdentifier, row
  MERGE (flight:Flight { id: flightIdentifier })
  // When merging, only use the id: flightIdentifier above, when creating use he below
  // eg: if already exists, don't change anything
  ON CREATE SET flight.date = row.Year + "-" + row.Month + "-" + row.DayofMonth,
                flight.airline = row.UniqueCarrier, flight.number = row.FlightNum, flight.departure = row.CRSDepTime,
                flight.arrival = row.CRSArrTime, flight.distance = row.Distance, flight.cancelled = row.Cancelled
  // create (ensure) two edges / relationships
  // relationship called ORIGIN from flight to origin
  // relationship called DESTINATION from flight to origin
  // eg: origin <- flight -> destination
  // NB: flight, origin an destination were 'bound' by the above MERGE statements "origin:Airport"
  MERGE (flight)-[:ORIGIN]->(origin)
  MERGE (flight)-[:DESTINATION]->(destination)

  // Added 10063 labels, created 30061 nodes, set 80056 properties, created 19998 relationships, completed after 51447 ms.

  // NB: some people got a NULL property value for code error
  //  WITH row WHERE row.Origin <> "" AND row.Dest <> ""
  //  WITH row WHERE not(row.Origin is null)AND not(row.Dest is null)
  // This was b/c in the example, the instructor was not referencing the raw github url

  MATCH (flight:Flight)
  WHERE flight.distance > 500
  RETURN flight


NB: by defualt, CSV loads every field as type String

Q: how do you ensure a connection or Cypher query is read only?  It looks like all cypher queries are read or write?

a  CREATE CONSTRAINT ON (a:Airport)
ASSERT a.code IS UNIQUE

PROFILE
 MATCH (flight:Flight)
 WHERE flight.cancelled = "0"
RETURN count(*)

 MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
RETURN count(*)

// profile runs the query
PROFILE
MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
 RETURN count(*)

// 10k db hits

// explain doesn't run
EXPLAIN
MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
 RETURN count(*)



// index the airport code and flight id for better perf
CREATE CONSTRAINT ON (a:Airport)
ASSERT a.code is UNIQUE

CREATE CONSTRAINT ON (f:Flight)
ASSERT f.id IS UNIQUE

PROFILE
MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
 RETURN count(*)

:schema // <-- shows the schema w/constraints


// DANGER (cascade delete)
match (n) detach delete n

// Wow, the import took ~50s the first time, now it's 1.4s

  LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/neo4j-contrib/training/master/modeling/data/flights_initial.csv" AS row
  MERGE (origin:Airport {code: row.Origin})
  MERGE (destination:Airport {code: row.Dest})
  WITH row.UniqueCarrier + row.FlightNum + "_" + row.Year + "-" + row.Month + "-" + row.DayofMonth + "_" + row.Origin + "_" + row.Dest AS flightIdentifier, row
  MERGE (flight:Flight { id: flightIdentifier })
  ON CREATE SET flight.date = row.Year + "-" + row.Month + "-" + row.DayofMonth,
                flight.airline = row.UniqueCarrier, flight.number = row.FlightNum, flight.departure = row.CRSDepTime,
                flight.arrival = row.CRSArrTime, flight.distance = row.Distance, flight.cancelled = row.Cancelled
  MERGE (flight)-[:ORIGIN]->(origin)
  MERGE (flight)-[:DESTINATION]->(destination)




Gene: Titan is HDFS based, not a full blown database.  My not be ACID compliant ...

################################################################################
# 2017-10-23T14:46:13Z - section 1 is completed



