# TODO: try https://github.com/gorillalabs/neo4j-clj for the BOLT driver
# TODO: how are we going to handle migrations tooling w/neo4j?
#       https://neo4j.com/blog/neo4j-real-world-performance/
# TODO: read: https://snowplowanalytics.com/blog/2014/07/28/explorations-in-analyzing-web-event-data-in-graph-databases/
# TODO: create an example showing how to create a procedure we can call from cypher (needs to be compiled to byte code with annotations)
# TODO: Michael Moore GraphAdvantage - Kafka Streaming into Neo4j
# https://db-engines.com/en/system/JanusGraph%3BNeo4j%3BTitan
#      http://www.objectivity.com/products/infinitegraph/
# https://www.opencypher.org/#resources
# recommended by Zach:
#  "Leveraging the Graph for Knowledge Architecture at NASA"
#   https://km.nasa.gov/wp-content/uploads/sites/3/2016/06/Meza-David.pdf
#   https://www.slideshare.net/neo4j/knowledge-architecture-graphing-your-knowledge
#   https://github.com/neo4j-contrib/training/tree/master/cloud
# TODO: what are the differences between the community version and the enterprise version?
#   see: https://neo4j.com/editions/
#  JVM Heap Tuning: https://www.elastic.co/blog/a-heap-of-trouble
# TODO[read]: "eager operator neo4j"
#   http://www.markhneedham.com/blog/2014/10/23/neo4j-cypher-avoiding-the-eager/
# TODO: terraform up our neo4j instance setup & cluster setup

Polyglot Persistence w/Neo4j

  Mark
  Praveena


Options for importing data into neo4j

  http://bit.ly/_import
   => https://docs.google.com/document/d/1dECIA-Q0qR8SLJu5WmFuR6Vl40PgnDtqb1CBvffgfL4/edit

   . first dataset from this doc is a stackoverflow sample

LOAD CSV via Cypher

* for well formatted csv data
* not as fast as other methods
* works w/transactions

Cypher and APOC

* Awesome Procedures On Cypher
* plugin mechanisim / procedures
* 'CALL PROC-NAME'
* 'FN-NAME()'
* APOC library brings togethr a set of common tools for import, eg: iterating &
  batching of transactions
* Hepers for: GraphML, JDBC, XML, JSON, & others.

Procedures

* write you own!  In Java, or anything that can compile into a jar
* place in plugins folder
* these will be via the transaction log
* fine grained access control, who can use, etc.

Driver via BOLT

* bolt is the protocol for connecting applications to neo4j service, custom
  wire protocol for graph data
* cypher queries -> stream of results
* .NET, Java, JavaScript and Python are officially supported
* Transactional, Batching & Parallelization possible
* Unofficial for PHP and Golang

./bin/neo4j-admin import

* fastest method -- skips the transaction log!
* initial import, a new database is created!
* db is offline during import, data must be specially prepared!
* cluster must be synchronized AFTER import!

Q[guy w/Airborne hat]: when will you support multiple databases per instance
>> NB: no schemas or multi-tenancy?
>> working group witin cypher, 'multigraph problem'
>> "at the db level, it's all designed for multiple databases'

NB: import is safe, if the destination exists, it'll refuse unless you specify `--force`

BatchInserter

* initial import or update (yay)
* application uses the batchloader API
* one of the faster import methods & more effort than the other approaches

neo4j ETL
* offline tool
* see: https://neo4j.com/blog/rdbms-neo4j-etl-tool/


Cypher and load CSV
  * https://neo4j.com/docs/cypher-refcard/current/

    LOAD CSV
    WITH HEADERS    // use first row as the header, rows are treated as a map
    FROM "url"      // file:// url relative to $NEO4J_HOME/import or http:// (??? or https://)
      AS row        // each line of CSV will be treated as a list of strings or map
                    //   NB: 'row' is a name we choose for the statement
    ...             // remainder of Cypher statement


    [USING PERIODIC COMMIT]  // optionally batch transactions

    ...
    AS row
    [FIELDTERMINATOR ";"] // optionally specify the delimiter
    ...             // remainder of Cypher statement



    LOAD CSV WITH HEADERS FROM "(file|http)://" AS row
    CREATE (:Label {property: row.header})
     MATCH (:Label {property: row.header})  // <- OR
     MERGE (:Label {property: row.header})  // <- OR

    // where :Label is the entity type eg: :ID(Person)


Jump to http://localhost:7474/browser/, then enter into the cypher box:

  :play https://guides.neo4j.com/import
  :clear

interactive tutorial / guide

  LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/neo4j-contrib/training/master/modeling/data/flights_initial.csv" AS row
  RETURN row
  LIMIT 5



  LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/neo4j-contrib/training/master/modeling/data/flights_initial.csv" AS row
  // merge the Airport (origin and dest) since that code is highly denormalized
  MERGE (origin:Airport {code: row.Origin})
  MERGE (destination:Airport {code: row.Dest})
  // NB: there is no unique identifier for a flight, so we'll attempt to make one here
  WITH row.UniqueCarrier + row.FlightNum + "_" + row.Year + "-" + row.Month + "-" + row.DayofMonth + "_" + row.Origin + "_" + row.Dest AS flightIdentifier, row
  MERGE (flight:Flight { id: flightIdentifier })
  // When merging, only use the id: flightIdentifier above, when creating use he below
  // eg: if already exists, don't change anything
  ON CREATE SET flight.date = row.Year + "-" + row.Month + "-" + row.DayofMonth,
                flight.airline = row.UniqueCarrier, flight.number = row.FlightNum, flight.departure = row.CRSDepTime,
                flight.arrival = row.CRSArrTime, flight.distance = row.Distance, flight.cancelled = row.Cancelled
  // create (ensure) two edges / relationships
  // relationship called ORIGIN from flight to origin
  // relationship called DESTINATION from flight to origin
  // eg: origin <- flight -> destination
  // NB: flight, origin an destination were 'bound' by the above MERGE statements "origin:Airport"
  MERGE (flight)-[:ORIGIN]->(origin)
  MERGE (flight)-[:DESTINATION]->(destination)

  // Added 10063 labels, created 30061 nodes, set 80056 properties, created 19998 relationships, completed after 51447 ms.

  // NB: some people got a NULL property value for code error
  //  WITH row WHERE row.Origin <> "" AND row.Dest <> ""
  //  WITH row WHERE not(row.Origin is null)AND not(row.Dest is null)
  // This was b/c in the example, the instructor was not referencing the raw github url

  MATCH (flight:Flight)
  WHERE flight.distance > 500
  RETURN flight


NB: by defualt, CSV loads every field as type String

Q: how do you ensure a connection or Cypher query is read only?  It looks like all cypher queries are read or write?

a  CREATE CONSTRAINT ON (a:Airport)
ASSERT a.code IS UNIQUE

PROFILE
 MATCH (flight:Flight)
 WHERE flight.cancelled = "0"
RETURN count(*)

 MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
RETURN count(*)

// profile runs the query
PROFILE
MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
 RETURN count(*)

// 10k db hits

// explain doesn't run
EXPLAIN
MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
 RETURN count(*)



// index the airport code and flight id for better perf
CREATE CONSTRAINT ON (a:Airport)
ASSERT a.code is UNIQUE

CREATE CONSTRAINT ON (f:Flight)
ASSERT f.id IS UNIQUE

PROFILE
MATCH (flight:Flight)
 WHERE not(flight.cancelled = "0")
 RETURN count(*)

:schema // <-- shows the schema w/constraints


// DANGER (cascade delete)
match (n) detach delete n

// Wow, the import took ~50s the first time, now it's 1.4s

  LOAD CSV WITH HEADERS FROM "https://raw.githubusercontent.com/neo4j-contrib/training/master/modeling/data/flights_initial.csv" AS row
  MERGE (origin:Airport {code: row.Origin})
  MERGE (destination:Airport {code: row.Dest})
  WITH row.UniqueCarrier + row.FlightNum + "_" + row.Year + "-" + row.Month + "-" + row.DayofMonth + "_" + row.Origin + "_" + row.Dest AS flightIdentifier, row
  MERGE (flight:Flight { id: flightIdentifier })
  ON CREATE SET flight.date = row.Year + "-" + row.Month + "-" + row.DayofMonth,
                flight.airline = row.UniqueCarrier, flight.number = row.FlightNum, flight.departure = row.CRSDepTime,
                flight.arrival = row.CRSArrTime, flight.distance = row.Distance, flight.cancelled = row.Cancelled
  MERGE (flight)-[:ORIGIN]->(origin)
  MERGE (flight)-[:DESTINATION]->(destination)




Gene: Titan is HDFS based, not a full blown database.  My not be ACID compliant ...

################################################################################
# 2017-10-23T14:46:13Z - section 1 is completed

BatchInserter or neo4j-admin import

* database has to be offline
* building a new database

$ brew install p7zip

$ bake run:neo4j start -path ./software/neo4j-community-3.2.6/data/databases/stackoverflow.db/


  https://github.com/jexp/neo4j-stackoverflow-import

MATCH (:User) RETURN count(*)

# NB: ok, how do you get a count of relations?
# http://bigdatums.net/2017/01/01/counting-relationships-in-neo4j/
MATCH (n)-[r]->() RETURN COUNT(r)


MATCH (u:User)
WITH u, size( (u)-[:POSTED]->() ) as posts
ORDER by posts DESC
LIMIT 10
RETURN u.name, posts


CREATE INDEX ON :Tag(tagId)


EXERCISE: load the badges data
  https://archive.org/download/stackexchange
    https://archive.org/download/stackexchange/stackoverflow.com-Badges.7z


## a few more samples the instructor was writing
call apoc.create.node(["Person", "Foo"], {})

with ["Person", "Foo"] as labels
call apoc.create.node(labels, {}) yield node return count(*)

Perf Tips:

* separate disk for input vs output
* compress the csv files
* more cores the better
* separate headers from data (why??)

neo4j-import parallelizes the csv import



https://github.com/jexp/neo4j-dataset-import
  Example of a custom tool for going from a custom format to csv then csv into neo4j-import


# section 3: Drivers via Bolt

* 1.4.2 [bolt protocol version] . [driver feature set] . [driver patch level]
* https://github.com/neo4j/neo4j-java-driver :: 1.4.4

SEE: https://github.com/neo4j-contrib/neo4j-apoc-procedures

FEEDBACK: watching Mark create & execute queries may be showing off neo4j,
          though I don't know what the class got from watching him do this when ad-hoc
          exploring the stackoverflow data set


SEE: https://github.com/neo4j-contrib


################################################################################
# 2017-10-23T17:28:55Z

* Dave Gordon
* Mark Needham
* Martin Furmanski
* Ryan Boyd

Q: has neo4j been tested w/Jepsen? >> doesn't look like it

  bit.ly/_neo4jcloud
    https://docs.google.com/document/d/1fSh_CWodS_BQKEx4ePQ5D9ndzFpQLLM6rdWUhN8o8eo/view

# 2017-10-23T17:36:01Z - they gathered topics (on postit notes) again :/

Recommended that we use an AWS account to follow along :/

* Deploying a Single Node Neo4j Server

https://github.com/neo4j-contrib/training/tree/master/cloud


FEEDBACK: first part of this .. installing a single node is just the ami or
super rudimentary setup, nice that it's straightforward, not much out of the
training here though.  Spent time waiting for Mark to start & stop the EC2
instances.

Machine Sizing: number of cores; type of disk; how much ram

Page Cache size:  Select the size based on
* size of store.db files
* the amount of growth you expect to see, eg: 20%

  default dbms.memory.pagecache.size=4096m

  # by default, it's dynamic based on how much ram the machine has

Q: is there a tuning guide?

David G. [Neo4j]: "generally graphs are going to fit in memory" <- ???

 pagecache is off heap

Heap Size
  Select size based on:
* size of queries
* amount of transaction state create by queries
* number of concurrent queries

Heap Size: rules of thumb

  Test with a production like workload
* start w/a heap of 8-12Gb
* run the workoad
  if big GC pauses then increase expirementally up to a max of 26 GB
* if there are still big GC pauses
  tune queries
  reduce the number of client threads hitting the server

NB: does this mean scaling horizontally or constraining the connection resources?

  watch out for eager operators, use 'profile' or 'explain' to find out!


2017-10-23T18:14:44Z Starting Causal Clustering

Product released in the 3.1 seriese
Neo4j HA was a predecesor for CC (Causal Clustering)

2 types of machines: core or replica
Core:
* small group
* implements cosensus commit
* responsiblie for data safety

they've seen clusters with up to 9 machines in the role of core

Replica:
* for massive query throughput
* read-only replicas
* not involved in consensu commit => less overhead
* disposable, suitable for auto-scaling

* raft based consensus protcol in CC for consensus commits
* paxos is for node membership
* they have an internal stress test, couldn't get jepsen to compile (???) so kinda gave up
  https://web.stanford.edu/~ouster/cgi-bin/papers/raft-atc14

  NB: I hope they try again, or hire Kingsbury to test out neo4j, the fact that they
      haven't might be a sign -- the agreement w/Jepsen is that results will be published
      regardless of the outcome.  That's a hard thing to face.


Deploying a Neo4j Causal Cluster
  HTAP: Hybrid Transactional / Analytical Processing

minikube - lets you run & test out kubernets on single-machine environments
(eg: a laptop).  Q: is this like docker-compose?




